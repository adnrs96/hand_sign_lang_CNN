{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 850M (CNMeM is disabled, cuDNN 5110)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/sign_mix_expedition3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model and doing a transfer of weights from vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is for extracting out the convolutional part of vgg16\n",
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in conv_layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(24, activation='softmax')\n",
    "        ]\n",
    "p=0.8\n",
    "bn_fc_layers = get_bn_da_layers(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hand_sign_model = Sequential(conv_layers + bn_fc_layers)\n",
    "hand_sign_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate batches for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1748 images belonging to 24 classes.\n",
      "Found 336 images belonging to 24 classes.\n",
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate batches for all our data\n",
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1748 images belonging to 24 classes.\n",
      "Found 336 images belonging to 24 classes.\n",
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get augmented training batches\n",
    "Do adjust data multiplication factor below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1748 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.15, \n",
    "                                 channel_shift_range=20, width_shift_range=0.15)\n",
    "da_batches = get_batches(path+'train', gen_t, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_trn_labels = np.concatenate([trn_labels]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1748"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_batches.nb_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try fitting to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6992/6992 [==============================] - 354s - loss: 0.6637 - acc: 0.7850 - val_loss: 0.2161 - val_acc: 0.9286\n",
      "Epoch 2/10\n",
      "6992/6992 [==============================] - 353s - loss: 0.6417 - acc: 0.7925 - val_loss: 0.2205 - val_acc: 0.9256\n",
      "Epoch 3/10\n",
      "6992/6992 [==============================] - 371s - loss: 0.6432 - acc: 0.7885 - val_loss: 0.2327 - val_acc: 0.9196\n",
      "Epoch 4/10\n",
      "6992/6992 [==============================] - 378s - loss: 0.6057 - acc: 0.8026 - val_loss: 0.2227 - val_acc: 0.9256\n",
      "Epoch 5/10\n",
      "6992/6992 [==============================] - 370s - loss: 0.6159 - acc: 0.8009 - val_loss: 0.2321 - val_acc: 0.9196\n",
      "Epoch 6/10\n",
      "6992/6992 [==============================] - 374s - loss: 0.5927 - acc: 0.8053 - val_loss: 0.2381 - val_acc: 0.9196\n",
      "Epoch 7/10\n",
      "6992/6992 [==============================] - 370s - loss: 0.6032 - acc: 0.8039 - val_loss: 0.2346 - val_acc: 0.9167\n",
      "Epoch 8/10\n",
      "6992/6992 [==============================] - 375s - loss: 0.5959 - acc: 0.8079 - val_loss: 0.2480 - val_acc: 0.9137\n",
      "Epoch 9/10\n",
      "6992/6992 [==============================] - 363s - loss: 0.5865 - acc: 0.8051 - val_loss: 0.2512 - val_acc: 0.9137\n",
      "Epoch 10/10\n",
      "6992/6992 [==============================] - 360s - loss: 0.6122 - acc: 0.8012 - val_loss: 0.2350 - val_acc: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04fe05c610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_sign_model.fit_generator(da_batches, samples_per_epoch=da_batches.nb_sample*4, nb_epoch=10,\n",
    "                validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hand_sign_model.save_weights(path+'models/da_conv8_1_at80.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do only when weights have been saved already\n",
    "hand_sign_model.load_weights(path+'models/da_conv8_1_at71.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets test test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_labels = {}\n",
    "for x,y in batches.class_indices.items():\n",
    "    final_labels[y]=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(path+'test', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = hand_sign_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.183358\n",
      "b 0.00023903\n",
      "c 0.000481377\n",
      "d 0.000301477\n",
      "e 0.0616198\n",
      "f 0.000167032\n",
      "g 0.000389257\n",
      "h 0.000382179\n",
      "i 0.00173031\n",
      "k 5.98134e-05\n",
      "l 1.44814e-05\n",
      "m 0.00887039\n",
      "n 0.01836\n",
      "o 0.003272\n",
      "p 0.00112842\n",
      "q 0.000388247\n",
      "r 0.000142838\n",
      "s 0.53325\n",
      "t 0.174555\n",
      "u 0.000123415\n",
      "v 8.04657e-06\n",
      "w 1.33755e-05\n",
      "x 0.00819007\n",
      "y 0.00295565\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(preds[0]):\n",
    "    print(final_labels[i], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "62481/62481 [==============================] - 4478s - loss: 0.8278 - acc: 0.7240 - val_loss: 0.2163 - val_acc: 0.9428\n"
     ]
    }
   ],
   "source": [
    "hand_sign_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=1,\n",
    "                validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "hand_sign_model.save_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'b',\n",
       " 2: 'c',\n",
       " 3: 'd',\n",
       " 4: 'e',\n",
       " 5: 'f',\n",
       " 6: 'g',\n",
       " 7: 'h',\n",
       " 8: 'i',\n",
       " 9: 'k',\n",
       " 10: 'l',\n",
       " 11: 'm',\n",
       " 12: 'n',\n",
       " 13: 'o',\n",
       " 14: 'p',\n",
       " 15: 'q',\n",
       " 16: 'r',\n",
       " 17: 's',\n",
       " 18: 't',\n",
       " 19: 'u',\n",
       " 20: 'v',\n",
       " 21: 'w',\n",
       " 22: 'x',\n",
       " 23: 'y'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labels = {}\n",
    "for x,y in batches.class_indices.items():\n",
    "    final_labels[y]=x\n",
    "final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
